import os
import re
import time
import json
import asyncio
import traceback
import logging
import hashlib
from datetime import datetime
from pathlib import Path
import gradio as gr
from dotenv import load_dotenv
from openai import OpenAI
from sentence_transformers import CrossEncoder  # Importando CrossEncoder para reranking local
import httpx

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("chat_client.log", mode='w')
    ]
)
logger = logging.getLogger("rag_chat")

# FastMCP v2 importa√ß√µes
from fastmcp import Client
from fastmcp.client.transports import StreamableHttpTransport
from langchain_community.chat_models import ChatOllama # Adicionar importa√ß√£o do ChatOllama
from langchain_core.messages import SystemMessage, HumanMessage # Para formatar mensagens para ChatOllama

# Vari√°veis de ambiente
load_dotenv()
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")

# Configura√ß√£o LLM via .env (cliente usa s√≠ntese, pode ser diferente dos servidores)
# Op√ß√µes: "API" (DeepSeek), "Ollama", "Anthropic"
LLM_CALL = os.getenv("LLM_CALL_CLIENT", "API")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "qwen3:30b")

def strip_think_tags(text: str) -> str:
    """Remove blocos <think>...</think> de modelos reasoning (ex: Qwen3, DeepSeek-R1)."""
    return re.sub(r'<think>.*?</think>\s*', '', text, flags=re.DOTALL).strip()

if LLM_CALL == "API" and not DEEPSEEK_API_KEY:
    logger.error("DEEPSEEK_API_KEY n√£o encontrada e LLM_CALL='API'. Verifique o arquivo .env")
elif LLM_CALL == "Anthropic" and not ANTHROPIC_API_KEY:
    logger.error("ANTHROPIC_API_KEY n√£o encontrada e LLM_CALL='Anthropic'. Verifique o arquivo .env")
elif LLM_CALL == "Ollama":
    logger.info("LLM_CALL configurado para 'Ollama'.")
elif LLM_CALL == "Anthropic":
    logger.info("LLM_CALL configurado para 'Anthropic' (Claude).")

# Cliente OpenAI para DeepSeek (ser√° usado se LLM_CALL == "API")
openai_client = None
if LLM_CALL == "API" and DEEPSEEK_API_KEY:
    openai_client = OpenAI(
        base_url="https://api.deepseek.com",
        api_key=DEEPSEEK_API_KEY
    )
elif LLM_CALL == "API" and not DEEPSEEK_API_KEY:
    logger.warning("LLM_CALL √© 'API', mas DEEPSEEK_API_KEY n√£o est√° definida. A s√≠ntese via API falhar√°.")

# Cliente Anthropic (ser√° usado se LLM_CALL == "Anthropic")
anthropic_client = None
if LLM_CALL == "Anthropic" and ANTHROPIC_API_KEY:
    try:
        import anthropic
        anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
        logger.info("Cliente Anthropic inicializado com sucesso.")
    except ImportError:
        logger.error("Biblioteca 'anthropic' n√£o instalada. Execute: pip install anthropic")
elif LLM_CALL == "Anthropic" and not ANTHROPIC_API_KEY:
    logger.warning("LLM_CALL √© 'Anthropic', mas ANTHROPIC_API_KEY n√£o est√° definida.")


# Configura√ß√µes dos servidores MCP
MCP_SERVERS = {
    "TELEBRAS": {"url": "http://localhost:8011/mcp/", "description": "Conhecimento TELEBRAS."},
    "CEITEC": {"url": "http://localhost:8009/mcp/", "description": "Conhecimento CEITEC."},
    "IMBEL": {"url": "http://localhost:8010/mcp/", "description": "Conhecimento IMBEL."}
}

# ===== Sistema de Autentica√ß√£o =====
USERS_FILE = Path(__file__).parent / "users.json"
CHAT_HISTORY_DIR = Path(__file__).parent / "chat_history"
CHAT_HISTORY_DIR.mkdir(exist_ok=True)

def load_users() -> dict:
    """Carrega usu√°rios do arquivo users.json."""
    try:
        with open(USERS_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        users = {}
        for u in data.get("users", []):
            users[u["username"]] = {
                "password": u["password"],
                "nome": u.get("nome", u["username"])
            }
        logger.info(f"Carregados {len(users)} usu√°rios do arquivo {USERS_FILE}")
        return users
    except FileNotFoundError:
        logger.warning(f"Arquivo {USERS_FILE} n√£o encontrado. Usando credenciais padr√£o.")
        return {"admin": {"password": "mgi2024", "nome": "Administrador"}}
    except Exception as e:
        logger.error(f"Erro ao carregar usu√°rios: {e}")
        return {"admin": {"password": "mgi2024", "nome": "Administrador"}}

AUTH_USERS = load_users()

def authenticate(username: str, password: str) -> bool:
    """Valida credenciais do usu√°rio."""
    if username in AUTH_USERS and AUTH_USERS[username]["password"] == password:
        logger.info(f"Login bem-sucedido: {username}")
        return True
    logger.warning(f"Tentativa de login falhou para: {username}")
    return False

def get_user_display_name(username: str) -> str:
    """Retorna o nome de exibi√ß√£o do usu√°rio."""
    if username in AUTH_USERS:
        return AUTH_USERS[username].get("nome", username)
    return username

def save_chat_history(username: str, history: list, session_file: str | None = None) -> str:
    """
    Salva o hist√≥rico de chat do usu√°rio em arquivo JSON.
    
    Args:
        username: Nome do usu√°rio
        history: Lista de mensagens do chat
        session_file: Nome do arquivo de sess√£o existente (opcional).
                      Se None, cria um novo arquivo.
    
    Returns:
        Nome do arquivo usado para salvar (para rastreamento da sess√£o)
    """
    try:
        user_dir = CHAT_HISTORY_DIR / username
        user_dir.mkdir(exist_ok=True)
        
        # Se temos um arquivo de sess√£o existente, usa ele; sen√£o, cria um novo
        if session_file:
            filepath = user_dir / session_file
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            session_file = f"chat_{timestamp}.json"
            filepath = user_dir / session_file
        
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump({
                "username": username,
                "timestamp": datetime.now().isoformat(),
                "messages": history
            }, f, ensure_ascii=False, indent=2)
        logger.info(f"Hist√≥rico salvo: {filepath}")
        return session_file
    except Exception as e:
        logger.error(f"Erro ao salvar hist√≥rico de {username}: {e}")
        return session_file or ""

def load_chat_sessions(username: str) -> list:
    """Carrega lista de sess√µes de chat do usu√°rio."""
    user_dir = CHAT_HISTORY_DIR / username
    if not user_dir.exists():
        return []
    sessions = []
    for f in sorted(user_dir.glob("chat_*.json"), reverse=True):
        try:
            with open(f, "r", encoding="utf-8") as fh:
                data = json.load(fh)
            first_msg = ""
            for msg in data.get("messages", []):
                if msg.get("role") == "user":
                    first_msg = msg["content"][:80] + ("..." if len(msg["content"]) > 80 else "")
                    break
            sessions.append({
                "file": f.name,
                "timestamp": data.get("timestamp", ""),
                "preview": first_msg or "Chat vazio",
                "message_count": len(data.get("messages", []))
            })
        except Exception:
            continue
    return sessions

def load_chat_session(username: str, filename: str) -> list:
    """Carrega uma sess√£o de chat espec√≠fica."""
    filepath = CHAT_HISTORY_DIR / username / filename
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data.get("messages", [])
    except Exception as e:
        logger.error(f"Erro ao carregar sess√£o {filename}: {e}")
        return []

def rename_chat_session(username: str, old_filename: str, new_name: str) -> tuple[bool, str]:
    """
    Renomeia uma sess√£o de chat.
    
    Args:
        username: Nome do usu√°rio
        old_filename: Nome atual do arquivo
        new_name: Novo nome desejado (sem extens√£o)
    
    Returns:
        tuple: (sucesso, novo_filename ou mensagem de erro)
    """
    if not new_name or not new_name.strip():
        return False, "Nome n√£o pode ser vazio"
    
    # Sanitizar o novo nome (remover caracteres inv√°lidos)
    safe_name = re.sub(r'[^\w\s\-_]', '', new_name.strip())[:50]
    if not safe_name:
        return False, "Nome inv√°lido ap√≥s sanitiza√ß√£o"
    
    user_dir = CHAT_HISTORY_DIR / username
    old_path = user_dir / old_filename
    
    if not old_path.exists():
        return False, "Arquivo n√£o encontrado"
    
    # Criar novo nome de arquivo (preservar timestamp se existir)
    timestamp_match = re.search(r'(\d{8}_\d{6})', old_filename)
    if timestamp_match:
        new_filename = f"{safe_name}_{timestamp_match.group(1)}.json"
    else:
        new_filename = f"{safe_name}.json"
    
    new_path = user_dir / new_filename
    
    # Verificar se j√° existe arquivo com esse nome
    if new_path.exists() and new_path != old_path:
        return False, "J√° existe um chat com esse nome"
    
    try:
        # Renomear o arquivo
        old_path.rename(new_path)
        logger.info(f"Chat renomeado: {old_filename} -> {new_filename}")
        return True, new_filename
    except Exception as e:
        logger.error(f"Erro ao renomear chat: {e}")
        return False, f"Erro ao renomear: {str(e)}"

# ===== Sistema de Gerenciamento de Tokens e Contexto =====
# Limites de tokens para DeepSeek (modelo deepseek-chat tem 64k de contexto)
MAX_CONTEXT_TOKENS = 50000  # Limite seguro para input (deixando espa√ßo para output)
COMPACTION_THRESHOLD = 0.80  # Compactar quando atingir 80% do limite
TOKENS_PER_CHAR = 0.25  # Estimativa: ~4 caracteres = 1 token para portugu√™s

def estimate_tokens(text: str) -> int:
    """Estima o n√∫mero de tokens em um texto (aproxima√ß√£o para portugu√™s)."""
    if not text:
        return 0
    return int(len(text) * TOKENS_PER_CHAR)

def estimate_history_tokens(history: list) -> int:
    """Estima o total de tokens no hist√≥rico de conversa."""
    total = 0
    for msg in history:
        content = msg.get("content", "")
        total += estimate_tokens(content)
        # Adicionar overhead por mensagem (role, formata√ß√£o)
        total += 4
    return total

def get_token_usage_percentage(history: list) -> float:
    """Retorna a porcentagem de tokens usados em rela√ß√£o ao limite."""
    tokens_used = estimate_history_tokens(history)
    return min((tokens_used / MAX_CONTEXT_TOKENS) * 100, 100.0)

def should_compact_history(history: list) -> bool:
    """Verifica se o hist√≥rico precisa ser compactado."""
    usage = get_token_usage_percentage(history)
    return usage >= (COMPACTION_THRESHOLD * 100)

def compact_history(history: list) -> tuple[list, str]:
    """
    Compacta o hist√≥rico de conversa, resumindo mensagens antigas.
    Mant√©m as √∫ltimas 4 mensagens intactas e resume o resto.
    
    Returns:
        tuple: (hist√≥rico compactado, resumo gerado)
    """
    if len(history) <= 6:
        return history, ""
    
    # Separar mensagens antigas das recentes
    messages_to_summarize = history[:-4]
    recent_messages = history[-4:]
    
    # Criar resumo das mensagens antigas
    summary_parts = []
    for msg in messages_to_summarize:
        role = "Usu√°rio" if msg.get("role") == "user" else "Assistente"
        content = msg.get("content", "")[:500]  # Limitar tamanho
        summary_parts.append(f"[{role}]: {content}")
    
    summary_text = "\n".join(summary_parts)
    
    # Criar prompt para resumo
    summary_prompt = f"""Resuma de forma MUITO concisa (m√°ximo 300 palavras) a seguinte conversa anterior, 
mantendo apenas os pontos essenciais, decis√µes tomadas e informa√ß√µes importantes mencionadas:

{summary_text}

Formato do resumo:
- T√≥picos principais discutidos
- Informa√ß√µes importantes mencionadas
- Contexto relevante para continuidade"""
    
    # Gerar resumo usando a LLM
    compacted_summary = ""
    try:
        if LLM_CALL == "Anthropic" and anthropic_client:
            response = anthropic_client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=500,
                system="Voc√™ √© um assistente que faz resumos concisos de conversas.",
                messages=[{"role": "user", "content": summary_prompt}]
            )
            compacted_summary = response.content[0].text
        elif LLM_CALL == "API" and openai_client:
            response = openai_client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "Voc√™ √© um assistente que faz resumos concisos de conversas."},
                    {"role": "user", "content": summary_prompt}
                ],
                temperature=0.3,
                max_tokens=500
            )
            compacted_summary = strip_think_tags(response.choices[0].message.content)
        elif LLM_CALL == "Ollama":
            ollama_llm = ChatOllama(model=OLLAMA_MODEL, temperature=0.3, num_gpu=1)
            response = ollama_llm.invoke([
                SystemMessage(content="Voc√™ √© um assistente que faz resumos concisos de conversas."),
                HumanMessage(content=summary_prompt)
            ])
            compacted_summary = strip_think_tags(response.content)
    except Exception as e:
        logger.error(f"Erro ao gerar resumo para compacta√ß√£o: {e}")
        # Fallback: criar resumo simples
        compacted_summary = f"[Resumo de {len(messages_to_summarize)} mensagens anteriores - contexto preservado]"
    
    # Criar hist√≥rico compactado
    compacted_history = [
        {"role": "system", "content": f"üìã **Resumo da conversa anterior:**\n{compacted_summary}"}
    ] + recent_messages
    
    logger.info(f"Hist√≥rico compactado: {len(history)} mensagens -> {len(compacted_history)} mensagens")
    return compacted_history, compacted_summary

def format_history_for_api(history: list) -> list:
    """Formata o hist√≥rico de chat para envio √† API."""
    formatted = []
    for msg in history:
        role = msg.get("role", "user")
        content = msg.get("content", "")
        if role in ["user", "assistant", "system"]:
            formatted.append({"role": role, "content": content})
    return formatted

# Configurar device do cliente (CrossEncoder)
CUDA_DEVICE_CLIENT = int(os.getenv("CUDA_DEVICE_CLIENT", "1"))
CLIENT_DEVICE = f'cuda:{CUDA_DEVICE_CLIENT}'

# Inicializar o CrossEncoder para reranking local
try:
    cross_encoder = CrossEncoder("cross-encoder/ms-marco-MiniLM-L6-v2", device=CLIENT_DEVICE)
    RERANKING_ENABLED = True
    logger.info(f"CrossEncoder para reranking local inicializado com sucesso em {CLIENT_DEVICE}")
except Exception as e:
    logger.warning(f"N√£o foi poss√≠vel inicializar o CrossEncoder: {e}")
    RERANKING_ENABLED = False

def rerank_results(query: str, results_dict: dict, top_n: int = 15) -> dict:
    """
    Reordena os resultados da consulta usando CrossEncoder para reranking.
    
    Args:
        query: A consulta original
        results_dict: Dicion√°rio de resultados para reordenar
        top_n: N√∫mero de fontes a manter ap√≥s reranking
        
    Returns:
        Dicion√°rio com resultados reordenados
    """
    if not RERANKING_ENABLED:
        return results_dict
    
    try:
        reranked_results = {}
        
        # Para cada servidor, aplicar reranking √†s fontes se poss√≠vel
        for server_name, result_data in results_dict.items():
            if not result_data or "error" in result_data or "content" not in result_data:
                reranked_results[server_name] = result_data
                continue
                
            content = result_data["content"]
            answer, sources = content.get("answer", ""), content.get("sources", [])
            
            if sources and len(sources) > top_n:
                # Preparar pares consulta-documento para o CrossEncoder
                doc_pairs = [(query, source) for source in sources]
                
                # Calcular scores com o CrossEncoder
                try:
                    scores = cross_encoder.predict(doc_pairs)
                    
                    # Ordenar fontes por score
                    sorted_pairs = sorted(zip(sources, scores), key=lambda x: x[1], reverse=True)
                    top_sources = [source for source, _ in sorted_pairs[:top_n]]
                    
                    # Atualizar as fontes no resultado
                    content["sources"] = top_sources
                    content["reranked"] = True
                    
                    logger.info(f"Reranking aplicado com sucesso para {server_name}: {len(sources)} -> {len(top_sources)} fontes")
                except Exception as e:
                    logger.error(f"Erro ao aplicar reranking para {server_name}: {e}")
                    # Manter as fontes originais em caso de erro
            
            reranked_results[server_name] = result_data
            
        return reranked_results
        
    except Exception as e:
        logger.error(f"Erro no processo de reranking: {e}")
        return results_dict  # Retornar resultados originais em caso de erro

async def parallel_mcp_query(query: str, max_results: int = 5, target_server: str = None) -> tuple[dict, list]:
    results = {}
    errors = []
    servers_to_query = [] # Inicializa a lista

    # Se um servidor espec√≠fico for solicitado, consultar apenas ele
    if target_server and target_server in MCP_SERVERS:
        servers_to_query = [target_server]
    else:
        servers_to_query = MCP_SERVERS.keys()
        
    async def query_server(server_name: str) -> tuple[str, dict]:
        server_config = MCP_SERVERS.get(server_name)
        if not server_config:
            msg = f"Configura√ß√£o para {server_name} n√£o encontrada."
            logger.error(msg)
            return server_name, {"error": msg}
        
        # Adicionar tentativas de reconex√£o
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(1, max_retries + 1):
            try:
                logger.info(f"Consultando {server_name} ({server_config['url']}) com query: '{query[:50]}...' (tentativa {attempt}/{max_retries})")
                start_time_query = time.time()
                
                # Remover o par√¢metro timeout que estava causando o erro
                transport = StreamableHttpTransport(url=server_config['url'])
                
                async with Client(transport=transport) as mcp_client_instance:
                    # Manter o timeout apenas na chamada wait_for
                    tools_list = await asyncio.wait_for(mcp_client_instance.list_tools(), timeout=30.0)
                    tool_name_to_call = f"query_{server_name.lower()}"
                    if not any(tool_obj.name == tool_name_to_call for tool_obj in tools_list):
                        available_tools_names = [tool_obj.name for tool_obj in tools_list]
                        msg = f"Ferramenta '{tool_name_to_call}' n√£o encontrada em {server_name}. Dispon√≠veis: {available_tools_names}"
                        logger.error(msg)
                        return server_name, {"error": msg}
                    
                    logger.info(f"Chamando '{tool_name_to_call}' em {server_name}...")
                    response_content_list = await asyncio.wait_for(
                        mcp_client_instance.call_tool(
                            name=tool_name_to_call,
                            arguments={"query": query, "max_results": max_results}
                        ),
                        timeout=300.0  # 5 minutos para primeira consulta (Ollama carrega modelo)
                    )
                    
                    # Processar resposta como antes...
                    response_data = None
                    if response_content_list:
                        content_item = response_content_list[0]
                        if hasattr(content_item, 'text'):
                            try:
                                response_data = json.loads(content_item.text)
                            except json.JSONDecodeError as je:
                                raw_text = content_item.text
                                msg = f"Erro ao decodificar JSON de {server_name}: {je}. Recebido: '{raw_text[:200]}...'"
                                logger.error(msg)
                                return server_name, {"error": msg, "raw_response": raw_text}
                        elif isinstance(content_item, dict):
                            response_data = content_item
                    
                    if isinstance(response_data, dict):
                        logger.info(f"Resposta recebida com sucesso de {server_name}.")
                        return server_name, {
                            "content": {
                                "answer": response_data.get("answer", "N/A"),
                                "sources": response_data.get("sources", []),
                                "processing_time": response_data.get("processing_time", time.time() - start_time_query),
                                "source_server": server_name
                            }
                        }
                    else:
                        raw_resp_str = str(response_content_list[0]) if response_content_list else "Lista de conte√∫do vazia"
                        msg = f"Formato de dados de resposta inesperado de {server_name}: {type(response_data)}. Conte√∫do bruto: '{raw_resp_str[:200]}...'"
                        logger.warning(msg)
                        return server_name, {"error": msg, "raw_response": raw_resp_str}
                    
            except asyncio.TimeoutError:
                msg = f"Timeout ao consultar {server_name} (tentativa {attempt}/{max_retries})."
                logger.error(msg)
                if attempt < max_retries:
                    logger.info(f"Tentando novamente em {retry_delay} segundos...")
                    await asyncio.sleep(retry_delay)
                    continue  # Tenta novamente
                return server_name, {"error": msg}
            
            except (httpx.RemoteProtocolError, httpx.ReadTimeout) as conn_err:
                msg = f"Erro de conex√£o com {server_name}: {type(conn_err).__name__} - {str(conn_err)} (tentativa {attempt}/{max_retries})"
                logger.error(msg)
                if attempt < max_retries:
                    logger.info(f"Tentando novamente em {retry_delay} segundos...")
                    await asyncio.sleep(retry_delay)
                    continue  # Tenta novamente
                return server_name, {"error": msg}
            
            except Exception as e:
                msg = f"Erro ao consultar {server_name}: {type(e).__name__} - {str(e)}"
                logger.error(msg)
                logger.error(traceback.format_exc())
                return server_name, {"error": msg}

        # Se chegou aqui, todas as tentativas falharam
        return server_name, {"error": f"Falha em todas as {max_retries} tentativas de conex√£o com {server_name}."}

    tasks = [query_server(name) for name in servers_to_query]
    if not tasks:
        logger.warning("Nenhum servidor MCP configurado para consulta.")
        return {}, ["Nenhum servidor configurado"]
    logger.info(f"Iniciando {len(tasks)} consultas MCP paralelas...")
    task_results_tuples = await asyncio.gather(*tasks, return_exceptions=True)
    for i, server_name_key in enumerate(servers_to_query):
        result_or_exc = task_results_tuples[i]
        if isinstance(result_or_exc, Exception):
            msg = f"Exce√ß√£o na tarefa de consulta para {server_name_key}: {result_or_exc}"
            logger.error(msg)
            results[server_name_key] = {"error": msg}
            errors.append(msg)
        elif isinstance(result_or_exc, tuple) and len(result_or_exc) == 2:
            actual_server_name, result_dict = result_or_exc
            results[actual_server_name] = result_dict
            if result_dict and "error" in result_dict:
                errors.append(f"{actual_server_name}: {result_dict['error']}")
            elif result_dict and "content" in result_dict:
                 logger.info(f"Consulta a {actual_server_name} bem-sucedida.")
            else:
                msg = f"Resposta inesperada ou malformada de {actual_server_name}: {result_dict}"
                logger.error(msg)
                results[actual_server_name] = {"error": msg}
                errors.append(msg)
        else:
            msg = f"Formato de resultado de tarefa inesperado para {server_name_key}: {result_or_exc}"
            logger.error(msg)
            results[server_name_key] = {"error": msg}
            errors.append(msg)
    
    # Aplicar reranking √†s fontes se o CrossEncoder estiver dispon√≠vel
    if RERANKING_ENABLED:
        logger.info("Aplicando reranking √†s fontes...")
        results = rerank_results(query, results)
            
    return results, errors

def create_consolidated_summary(query: str, results_dict: dict, chat_history: list = None) -> str:
    logger.info(f"Criando resumo consolidado para query: '{query[:50]}...'")
    valid_responses = []
    all_sources_dict = {} # Initialize as a dictionary
    
    # Detectar se a pergunta √© espec√≠fica para uma empresa
    query_lower = query.lower()
    target_company = None
    for company in MCP_SERVERS.keys():
        if company.lower() in query_lower:
            target_company = company
            break
        
    for server_name, result_data in results_dict.items():
        if not result_data or "error" in result_data or "content" not in result_data:
            error_info = result_data.get('error', 'Dados ausentes') if result_data else 'Nulo'
            logger.warning(f"Ignorando {server_name}: {error_info}")
            continue
        answer = result_data["content"].get("answer")
        if not answer:
            logger.warning(f"Resposta vazia de {server_name}, ignorando.")
            continue
        valid_responses.append({"server": server_name, "answer": answer})
        
        # Organizar fontes por empresa
        sources = result_data["content"].get("sources", [])
        if sources:
            all_sources_dict[server_name] = sources # Now using dictionary assignment
    if not valid_responses:
        logger.error("Nenhuma resposta v√°lida para sintetizar.")
        return "N√£o foi poss√≠vel obter informa√ß√µes relevantes."
    
    logger.info(f"Sintetizando {len(valid_responses)} respostas usando {LLM_CALL}...")
    
    # Prompt aprimorado para s√≠ntese de alta qualidade
    system_prompt_content = """
Voc√™ √© um analista especializado em empresas estatais brasileiras (TELEBRAS, CEITEC, IMBEL), com expertise em s√≠ntese de informa√ß√µes complexas de m√∫ltiplas fontes.

**SEU PAPEL:**
- Sintetizar respostas de 3 bases de conhecimento especializadas
- Produzir an√°lises coesas, precisas e bem estruturadas
- Manter rigor t√©cnico e clareza na comunica√ß√£o

**REGRA CR√çTICA DE SEPARA√á√ÉO DE EMPRESAS (OBRIGAT√ìRIO):**

‚ö†Ô∏è QUANDO O USU√ÅRIO PERGUNTAR ESPECIFICAMENTE SOBRE **UMA** EMPRESA:
- Se perguntar sobre **IMBEL**: N√ÉO mencione NADA sobre CEITEC ou TELEBRAS. N√ÉO fale do mercado da CEITEC. N√ÉO fale do mercado da TELEBRAS. N√ÉO inclua not√≠cias da CEITEC. N√ÉO inclua not√≠cias da TELEBRAS. IGNORE completamente qualquer informa√ß√£o das outras duas empresas.
- Se perguntar sobre **CEITEC**: N√ÉO mencione NADA sobre IMBEL ou TELEBRAS. N√ÉO fale do mercado da IMBEL. N√ÉO fale do mercado da TELEBRAS. N√ÉO inclua not√≠cias da IMBEL. N√ÉO inclua not√≠cias da TELEBRAS. IGNORE completamente qualquer informa√ß√£o das outras duas empresas.
- Se perguntar sobre **TELEBRAS**: N√ÉO mencione NADA sobre IMBEL ou CEITEC. N√ÉO fale do mercado da IMBEL. N√ÉO fale do mercado da CEITEC. N√ÉO inclua not√≠cias da IMBEL. N√ÉO inclua not√≠cias da CEITEC. IGNORE completamente qualquer informa√ß√£o das outras duas empresas.

üö´ O QUE N√ÉO FAZER (LISTA EXPL√çCITA):
- N√ÉO adicione "contexto" de outras empresas quando a pergunta for sobre uma espec√≠fica
- N√ÉO fa√ßa compara√ß√µes n√£o solicitadas entre empresas
- N√ÉO mencione "enquanto isso, na empresa X..." ou "por outro lado, a empresa Y..."
- N√ÉO inclua dados de mercado, financeiros ou not√≠cias de empresas n√£o perguntadas
- N√ÉO "complemente" a resposta com informa√ß√µes de outras empresas
- N√ÉO sugira que o usu√°rio "tamb√©m pode se interessar" por outra empresa na resposta principal

‚úÖ A √öNICA exce√ß√£o √© quando o usu√°rio EXPLICITAMENTE pedir compara√ß√£o entre empresas ou fizer uma pergunta geral sobre "as estatais" ou "todas as empresas".

**INSTRU√á√ïES DE ESTRUTURA√á√ÉO:**

1. **Para perguntas sobre UMA empresa:**
   - Foque EXCLUSIVAMENTE e UNICAMENTE na empresa mencionada
   - IGNORE COMPLETAMENTE informa√ß√µes de outras empresas (mesmo que estejam dispon√≠veis nos dados)
   - Estruture: Introdu√ß√£o breve ‚Üí An√°lise detalhada ‚Üí Conclus√£o
   - Se os dados das FONTES inclu√≠rem informa√ß√µes de outras empresas, DESCARTE essas informa√ß√µes

2. **Para perguntas comparativas ou gerais (SOMENTE quando explicitamente solicitado):**
   - Organize por empresa com subt√≠tulos claros (## EMPRESA)
   - Ap√≥s cobrir todas, adicione se√ß√£o "### An√°lise Comparativa" (se relevante)
   - Destaque diferen√ßas, similaridades e contextos √∫nicos

3. **Para perguntas t√©cnicas/financeiras:**
   - Use terminologia precisa (EBITDA, CAPEX, ROI, etc.)
   - Apresente dados quantitativos quando dispon√≠veis
   - Inclua contexto temporal ("em 2023", "no √∫ltimo tri√™nio")

4. **Para perguntas sobre projetos/cronogramas:**
   - Estruture cronologicamente
   - Destaque marcos importantes, status atual e previs√µes
   - Mencione riscos ou desafios identificados

**REGRAS DE CITA√á√ÉO:**
- Atribua cada informa√ß√£o √† empresa fonte ("Segundo dados da TELEBRAS...")
- Para dados espec√≠ficos, cite diretamente: "A CEITEC reportou..."
- N√£o invente dados nem misture informa√ß√µes de fontes diferentes

**FORMATA√á√ÉO:**
- Use Markdown: t√≠tulos (##), listas, **negrito** para √™nfase
- Par√°grafos concisos (3-5 linhas)
- Listas para m√∫ltiplos itens

**LIMITA√á√ïES:**
- Se a pergunta n√£o relacionar-se √†s empresas, responda: "Esta pergunta est√° fora do escopo. Posso ajudar com informa√ß√µes sobre TELEBRAS, CEITEC ou IMBEL."
- Se faltar informa√ß√£o: "Os dados dispon√≠veis n√£o cobrem [aspecto X]. Posso detalhar [aspecto Y]."

**TOM:**
Profissional, objetivo, anal√≠tico. Evite prolixidade, mas garanta completude.

**PROIBI√á√ïES ABSOLUTAS:**
- NUNCA mencione erros de servidores, falhas de conex√£o, timeouts ou problemas t√©cnicos internos na resposta.
- NUNCA exiba mensagens como "servidor com erro", "falha na comunica√ß√£o", "timeout" ou qualquer informa√ß√£o t√©cnica de infraestrutura.
- Se uma fonte n√£o retornou dados, simplesmente ignore-a e responda com as fontes dispon√≠veis, sem mencionar a aus√™ncia.
- NUNCA misture informa√ß√µes de empresas diferentes quando a pergunta for sobre UMA empresa espec√≠fica.
- NUNCA adicione "informa√ß√µes complementares" de CEITEC/TELEBRAS quando perguntarem sobre IMBEL.
- NUNCA adicione "informa√ß√µes complementares" de IMBEL/TELEBRAS quando perguntarem sobre CEITEC.
- NUNCA adicione "informa√ß√µes complementares" de IMBEL/CEITEC quando perguntarem sobre TELEBRAS.
- NUNCA fa√ßa compara√ß√µes entre empresas a menos que o usu√°rio PE√áA EXPLICITAMENTE.
- NUNCA inclua not√≠cias, mercado ou dados de empresas n√£o mencionadas na pergunta do usu√°rio.

**ENCERRAMENTO OBRIGAT√ìRIO:**
Ao final de TODA resposta, voc√™ DEVE incluir uma se√ß√£o de acompanhamento. Use o formato:

---

**Consigo ajudar em algo mais, como por exemplo:**
- [Sugest√£o 1 relacionada ao tema da pergunta ‚Äî ex: aprofundar algum ponto mencionado]
- [Sugest√£o 2 ‚Äî ex: explicar algum termo t√©cnico que apareceu na resposta]
- [Sugest√£o 3 ‚Äî ex: comparar com outra empresa ou explorar um aspecto diferente]

As sugest√µes devem ser ESPEC√çFICAS e CONTEXTUAIS ao que foi perguntado e respondido, nunca gen√©ricas. Ofere√ßa explica√ß√µes de termos t√©cnicos, compara√ß√µes entre empresas, detalhamentos de dados mencionados, ou explora√ß√µes de temas adjacentes.
    """
    context_str = "\n\n".join([f"FONTE {r['server']}:\n{r['answer']}" for r in valid_responses])
    user_prompt_content = f"PERGUNTA: {query}\n\nDADOS DAS FONTES:\n{context_str}\n\nRESPOSTA SINTETIZADA:"
    
    synthesized_answer = ""
    
    # Construir mensagens incluindo hist√≥rico de conversa (se existir)
    api_messages = [{"role": "system", "content": system_prompt_content}]
    
    # Adicionar hist√≥rico de conversa para manter contexto
    if chat_history:
        history_for_api = format_history_for_api(chat_history)
        # Filtrar mensagens de sistema duplicadas e adicionar hist√≥rico
        for msg in history_for_api:
            if msg["role"] != "system":  # Evitar m√∫ltiplos system prompts
                api_messages.append(msg)
        logger.info(f"Incluindo {len(history_for_api)} mensagens de hist√≥rico no contexto")
    
    # Adicionar a pergunta atual
    api_messages.append({"role": "user", "content": user_prompt_content})

    try:
        if LLM_CALL == "Anthropic":
            if not anthropic_client:
                raise ValueError("Cliente Anthropic n√£o inicializado. Verifique ANTHROPIC_API_KEY.")
            logger.info(f"Enviando para s√≠ntese LLM via Anthropic (Claude) com {len(api_messages)} mensagens...")
            # Separar system message das demais para API Anthropic
            system_content = ""
            anthropic_messages = []
            for msg in api_messages:
                if msg["role"] == "system":
                    system_content += msg["content"] + "\n"
                else:
                    anthropic_messages.append({"role": msg["role"], "content": msg["content"]})
            
            api_response = anthropic_client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=6000,
                system=system_content.strip(),
                messages=anthropic_messages
            )
            synthesized_answer = api_response.content[0].text
        
        elif LLM_CALL == "API":
            if not openai_client:
                raise ValueError("Cliente OpenAI (DeepSeek API) n√£o inicializado. Verifique DEEPSEEK_API_KEY.")
            logger.info(f"Enviando para s√≠ntese LLM via API (DeepSeek) com {len(api_messages)} mensagens...")
            api_response = openai_client.chat.completions.create(
                model="deepseek-chat", 
                messages=api_messages,
                temperature=1.0, 
                max_tokens=6000
            )
            synthesized_answer = strip_think_tags(api_response.choices[0].message.content)
        
        elif LLM_CALL == "Ollama":
            logger.info(f"Enviando para s√≠ntese LLM via Ollama ({OLLAMA_MODEL}) com {len(api_messages)} mensagens...")
            ollama_llm = ChatOllama(model=OLLAMA_MODEL, temperature=1.0, num_gpu=1)
            # Converter para formato LangChain
            messages_for_ollama = []
            for msg in api_messages:
                if msg["role"] == "system":
                    messages_for_ollama.append(SystemMessage(content=msg["content"]))
                elif msg["role"] == "user":
                    messages_for_ollama.append(HumanMessage(content=msg["content"]))
                elif msg["role"] == "assistant":
                    from langchain_core.messages import AIMessage
                    messages_for_ollama.append(AIMessage(content=msg["content"]))
            response_ollama = ollama_llm.invoke(messages_for_ollama)
            synthesized_answer = strip_think_tags(response_ollama.content)
        
        else:
            logger.error(f"Valor de LLM_CALL ('{LLM_CALL}') n√£o reconhecido. N√£o foi poss√≠vel sintetizar.")
            raise ValueError(f"Configura√ß√£o LLM_CALL inv√°lida: {LLM_CALL}")

        logger.info(f"S√≠ntese LLM gerada ({len(synthesized_answer)} chars).")
        
        # Processar o dicion√°rio de fontes em uma lista plana para exibi√ß√£o
        # flat_all_sources = []
        # for server_key in all_sources_dict:
        #     for source_item in all_sources_dict[server_key]:
        #         flat_all_sources.append(f"{server_key}: {source_item}") # Add server prefix to source

        # if flat_all_sources:
        #     unique_sources = sorted(list(set(flat_all_sources)))[:15]
        #     return f"{synthesized_answer}\n\n**Fontes:**\n" + "\n".join(f"- {s}" for s in unique_sources)
        return synthesized_answer

    except Exception as e:
        logger.error(f"Erro na s√≠ntese LLM ({LLM_CALL}): {e}", exc_info=True)
        fallback_answer = "**Respostas Individuais:**\n" + "\n".join(f"**{r['server']}:**\n{r['answer']}" for r in valid_responses)
        
        # Processar o dicion√°rio de fontes em uma lista plana para exibi√ß√£o
        # Fallback para fontes
        # flat_all_sources_fallback = []
        # for server_key_fb in all_sources_dict:
        #     for source_item_fb in all_sources_dict[server_key_fb]:
        #         flat_all_sources_fallback.append(f"{server_key_fb}: {source_item_fb}")

        # if flat_all_sources_fallback:
        #      fallback_answer += "\n\n**Fontes:**\n" + "\n".join(f"- {s}" for s in sorted(list(set(flat_all_sources_fallback)))[:10])
        return fallback_answer

def detect_company_in_query(query: str) -> str:
    """
    Detecta se a consulta √© espec√≠fica para uma empresa.
    
    Args:
        query: A consulta do usu√°rio.
        
    Returns:
        str: Nome da empresa detectada ou None se nenhuma for detectada.
    """
    query_lower = query.lower()
    
    # Palavras-chave espec√≠ficas para cada empresa
    company_keywords = {
        "CEITEC": ["ceitec", "semicondutores", "chips", "circuitos integrados", "rfid"],
        "IMBEL": ["imbel", "material b√©lico", "defesa", "armamentos", "muni√ß√µes", "explosivos"],
        "TELEBRAS": ["telebras", "telecomunica√ß√µes", "internet", "sat√©lite", "sgdc", "banda larga"]
    }
    
    # Verificar men√ß√µes expl√≠citas √†s empresas
    for company, keywords in company_keywords.items():
        for keyword in keywords:
            if keyword in query_lower:
                return company
    
    return None


async def async_rag_mcp_response(message: str, history: list, mode: str = "aggregated") -> str:
    logger.info(f"Processando consulta (modo: {mode}): '{message[:50]}...'")
    start_time_main = time.time()
    results_data, errors_list = await parallel_mcp_query(message)
    final_response_str = ""

    if not results_data and errors_list:
        return f"Erro: Falha na comunica√ß√£o com servidores. Detalhes: {'; '.join(errors_list)}"
    elif not results_data:
         return "Erro: Nenhum servidor respondeu."

    all_failed_or_empty = all(
        not rd or "content" not in rd or ("error" in rd) for rd in results_data.values()
    )
    if all_failed_or_empty:
        error_msgs = "; ".join(errors_list) if errors_list else "Respostas vazias/malformadas."
        return f"Erro ao consultar bases: {error_msgs}"

    if mode == "aggregated":
        final_response_str = create_consolidated_summary(message, results_data, chat_history=history)
    else:
        if mode in results_data and results_data[mode] and "content" in results_data[mode]:
            content = results_data[mode]["content"]
            answer, sources, proc_time = content.get("answer", "Sem resposta."), content.get("sources", []), content.get("processing_time", 0)
            final_response_str = f"{answer}"
            # if sources: final_response_str += "\n\n**Fontes:**\n" + "\n".join(f"- {s}" for s in sorted(list(set(sources)))[:5])
            final_response_str += f"\n\n[{mode}, {proc_time:.2f}s]"
        elif mode in results_data and results_data[mode] and "error" in results_data[mode]:
            final_response_str = f"Erro ({mode}): {results_data[mode]['error']}"
        else:
            final_response_str = f"{mode} n√£o dispon√≠vel ou resposta inv√°lida."
    
    processing_time_total = time.time() - start_time_main
    logger.info(f"Processamento total da consulta: {processing_time_total:.2f}s.")
    
    # Logar erros internamente, mas NUNCA expor ao usu√°rio
    if errors_list:
        logger.warning(f"Erros internos (n√£o exibidos ao usu√°rio): {', '.join(errors_list)}")
    
    return final_response_str

async def rag_aggregated_response_async(message, history):
    return await async_rag_mcp_response(message, history, "aggregated")

# Deixando as fun√ß√µes espec√≠ficas comentadas para simplificar a interface inicial
# async def rag_telebras_response_async(message, history):
#     return await async_rag_mcp_response(message, history, "TELEBRAS")
# async def rag_ceitec_response_async(message, history):
#     return await async_rag_mcp_response(message, history, "CEITEC")
# async def rag_imbel_response_async(message, history):
#     return await async_rag_mcp_response(message, history, "IMBEL")

async def check_server_availability(name: str, url: str) -> tuple[bool, list[str] | str]:
    logger.info(f"Verificando servidor {name} em {url}")
    try:
        # Remover o par√¢metro timeout daqui tamb√©m
        transport = StreamableHttpTransport(url=url)
        async with Client(transport=transport) as client_check:
            try:
                tools = await asyncio.wait_for(client_check.list_tools(), timeout=10.0)
                tool_names = [tool.name for tool in tools]
                logger.info(f"Servidor {name} OK. Ferramentas: {', '.join(tool_names)}")
                return True, tool_names
            except (httpx.HTTPStatusError, RuntimeError) as server_err:
                msg = f"Erro do servidor MCP ({url}): {type(server_err).__name__} - {server_err}"
                logger.error(msg)
                return False, msg
    except asyncio.TimeoutError:
        msg = f"Timeout MCP ({url})"
        logger.error(msg)
        return False, msg
    except Exception as e:
        msg = f"Erro MCP ({url}): {type(e).__name__} - {str(e)}"
        logger.error(msg, exc_info=True)
        return False, msg

def setup_and_launch_gradio():
    with gr.Blocks(title="Chat RAG MGI", theme=gr.themes.Soft(), css="""
        * {
            font-family: Arial, Helvetica, sans-serif !important;
        }
        .user-header { 
            display: flex; 
            justify-content: space-between; 
            align-items: center; 
            padding: 8px 16px; 
            background: linear-gradient(135deg, #1a5276, #2e86c1);
            border-radius: 8px; 
            margin-bottom: 12px;
            color: white;
        }
        .user-header span { font-size: 14px; }
        .user-header .username { font-weight: bold; font-size: 15px; }
        .history-item {
            padding: 8px 12px;
            margin: 4px 0;
            border-radius: 6px;
            border: 1px solid #e0e0e0;
            cursor: pointer;
            font-size: 13px;
        }
        .history-item:hover { background: #f0f4f8; }
        .logout-btn {
            background: rgba(255,255,255,0.2) !important;
            border: 1px solid rgba(255,255,255,0.4) !important;
            color: white !important;
            padding: 4px 12px !important;
            border-radius: 4px !important;
            font-size: 13px !important;
            min-width: auto !important;
        }
        .logout-btn:hover {
            background: rgba(255,255,255,0.3) !important;
        }
    """) as demo:
        
        # Header com info do usu√°rio e logout
        with gr.Row(elem_classes="user-header"):
            user_display = gr.Markdown("")
            logout_btn = gr.Button("Sair", elem_classes="logout-btn", size="sm", scale=0)
        
        # Fun√ß√£o de logout (recarrega a p√°gina para for√ßar novo login)
        logout_btn.click(
            fn=None,
            js="() => { window.location.href = window.location.pathname; }"
        )
        
        gr.Markdown("# Chat RAG Unificado - MGI")
        gr.Markdown("Fa√ßa uma pergunta para consultar as bases de conhecimento TELEBRAS, CEITEC e IMBEL.")
        
        with gr.Row():
            with gr.Column(scale=7):
                chatbot = gr.Chatbot(
                    height=600, 
                    label="Chat Consolidado", 
                    type='messages',
                    show_copy_button=True  # Bot√£o de copiar em cada mensagem
                )
                query_input = gr.Textbox(placeholder="Digite sua pergunta...", container=False)
            
            with gr.Column(scale=3):
                company_radio = gr.Radio(
                    choices=["Todas", "TELEBRAS", "CEITEC", "IMBEL"],
                    label="Empresa espec√≠fica (opcional)",
                    value="Todas"
                )
                
                gr.Markdown("---")
                
                # Indicador de uso de tokens
                gr.Markdown("### Uso do Contexto")
                token_progress = gr.Slider(
                    minimum=0, maximum=100, value=0, 
                    label="Capacidade da conversa",
                    interactive=False,
                    info="Quando chegar a 80%, a conversa ser√° resumida automaticamente para liberar espa√ßo."
                )
                token_status = gr.Markdown("üü¢ 0% - Conversa iniciada")
                
                gr.Markdown("---")
                gr.Markdown("### Hist√≥rico de Chats")
                
                new_chat_btn = gr.Button("üÜï Novo Chat", variant="primary", size="sm")
                save_chat_btn = gr.Button("üíæ Salvar Chat", variant="secondary", size="sm")
                
                history_list = gr.Dropdown(
                    label="Conversas anteriores",
                    choices=[],
                    interactive=True
                )
                load_chat_btn = gr.Button("üìÇ Carregar Conversa", size="sm")
                
                with gr.Row():
                    rename_input = gr.Textbox(
                        placeholder="Novo nome...",
                        container=False,
                        scale=3,
                        max_lines=1
                    )
                    rename_btn = gr.Button("‚úèÔ∏è", size="sm", scale=1)
        
        # State para rastrear o arquivo da sess√£o atual
        current_session_file = gr.State(value=None)
        
        # Exibir nome do usu√°rio logado no header
        def show_user_info(request: gr.Request):
            if request and request.username:
                display_name = get_user_display_name(request.username)
                return f"üë§ **{display_name}** ({request.username})"
            return "üë§ N√£o identificado"
        
        demo.load(show_user_info, inputs=None, outputs=user_display)
        
        # Carregar lista de sess√µes ao abrir
        def load_user_sessions(request: gr.Request):
            if request and request.username:
                sessions = load_chat_sessions(request.username)
                choices = [(f"{s['preview']} ({s['timestamp'][:10]})", s['file']) for s in sessions]
                return gr.Dropdown(choices=choices)
            return gr.Dropdown(choices=[])
        
        demo.load(load_user_sessions, inputs=None, outputs=history_list)
        
        # Novo chat
        def new_chat():
            # Retorna None para session_file, indicando que √© uma nova conversa
            return [], "", 0, "üü¢ 0% - Conversa iniciada", None
        
        new_chat_btn.click(new_chat, outputs=[chatbot, query_input, token_progress, token_status, current_session_file])
        
        # Salvar chat
        def save_current_chat(history: list, session_file: str, request: gr.Request):
            if request and request.username and history:
                new_session_file = save_chat_history(request.username, history, session_file)
                sessions = load_chat_sessions(request.username)
                choices = [(f"{s['preview']} ({s['timestamp'][:10]})", s['file']) for s in sessions]
                return gr.Dropdown(choices=choices), new_session_file
            return gr.Dropdown(choices=[]), session_file
        
        save_chat_btn.click(save_current_chat, inputs=[chatbot, current_session_file], outputs=[history_list, current_session_file])
        
        def get_token_status_display(percentage: float, was_compacted: bool = False) -> str:
            """Gera o texto de status baseado na porcentagem de tokens."""
            if was_compacted:
                return f"üîÑ {percentage:.0f}% - Conversa foi resumida automaticamente"
            elif percentage < 50:
                return f"üü¢ {percentage:.0f}% - Amplo espa√ßo dispon√≠vel"
            elif percentage < 80:
                return f"üü° {percentage:.0f}% - Moderado"
            else:
                return f"üü† {percentage:.0f}% - Pr√≥ximo do limite (ser√° resumido em breve)"
        
        # Carregar conversa anterior
        def load_previous_chat(selected_file: str, request: gr.Request):
            if request and request.username and selected_file:
                messages = load_chat_session(request.username, selected_file)
                token_percentage = get_token_usage_percentage(messages)
                # Retorna tamb√©m o arquivo de sess√£o para continuar editando o mesmo chat
                return messages, token_percentage, get_token_status_display(token_percentage), selected_file
            return [], 0, "üü¢ 0% - Conversa iniciada", None
        
        load_chat_btn.click(load_previous_chat, inputs=[history_list], outputs=[chatbot, token_progress, token_status, current_session_file])
        
        # Renomear chat
        def rename_selected_chat(selected_file: str, new_name: str, request: gr.Request):
            if not request or not request.username:
                gr.Warning("Usu√°rio n√£o identificado")
                return gr.Dropdown(choices=[]), ""
            
            if not selected_file:
                gr.Warning("Selecione uma conversa para renomear")
                return gr.Dropdown(choices=[]), new_name
            
            if not new_name or not new_name.strip():
                gr.Warning("Digite um nome para a conversa")
                return gr.Dropdown(choices=[]), new_name
            
            success, result = rename_chat_session(request.username, selected_file, new_name)
            
            if success:
                gr.Info(f"Conversa renomeada com sucesso!")
                # Atualizar lista de sess√µes
                sessions = load_chat_sessions(request.username)
                choices = [(f"{s['preview']} ({s['timestamp'][:10]})", s['file']) for s in sessions]
                return gr.Dropdown(choices=choices, value=result), ""
            else:
                gr.Warning(f"Erro: {result}")
                sessions = load_chat_sessions(request.username)
                choices = [(f"{s['preview']} ({s['timestamp'][:10]})", s['file']) for s in sessions]
                return gr.Dropdown(choices=choices), new_name
        
        rename_btn.click(rename_selected_chat, inputs=[history_list, rename_input], outputs=[history_list, rename_input])
        
        async def process_query(message: str, history: list, company: str, session_file: str, request: gr.Request):
            username = request.username if request else "anonymous"
            logger.info(f"[{username}] Nova consulta: '{message[:50]}...' (sess√£o: {session_file})")
            
            was_compacted = False
            
            # Verificar se precisa compactar o hist√≥rico ANTES de adicionar a nova mensagem
            if should_compact_history(history):
                logger.info(f"[{username}] Hist√≥rico pr√≥ximo do limite, compactando...")
                history, summary = compact_history(history)
                was_compacted = True
                if summary:
                    logger.info(f"[{username}] Hist√≥rico compactado. Resumo: {summary[:100]}...")
            
            # Modificar a consulta com base na empresa selecionada
            enhanced_query = message
            if company != "Todas":
                enhanced_query = f"[{company}] {enhanced_query}"
            
            # Adiciona a mensagem do usu√°rio ao hist√≥rico no formato correto
            history.append({"role": "user", "content": message})
            
            # Passar o hist√≥rico para manter contexto da conversa
            bot_response_string = await async_rag_mcp_response(enhanced_query, history, "aggregated")
            
            # Adiciona a resposta do bot ao hist√≥rico no formato correto
            history.append({"role": "assistant", "content": bot_response_string})
            
            # Calcular uso de tokens ap√≥s a resposta
            token_percentage = get_token_usage_percentage(history)
            token_status_text = get_token_status_display(token_percentage, was_compacted)
            
            # Auto-salvar chat (mantendo o mesmo arquivo de sess√£o)
            new_session_file = session_file
            if username != "anonymous":
                new_session_file = save_chat_history(username, history, session_file)
            
            # Retorna o hist√≥rico atualizado, indicadores de tokens e o arquivo de sess√£o
            return history, token_percentage, token_status_text, new_session_file
        
        submit_btn = gr.Button("Enviar")
        submit_btn.click(
            process_query,
            inputs=[query_input, chatbot, company_radio, current_session_file],
            outputs=[chatbot, token_progress, token_status, current_session_file]
        )

    # Par√¢metros de autentica√ß√£o
    auth_params = {
        "auth": authenticate,
        "auth_message": "üîê Chat RAG MGI - Sistema de Consulta\n\nInsira suas credenciais para acessar o sistema."
    }
    
    env_port = os.getenv("GRADIO_SERVER_PORT")
    port_to_use = 0
    if env_port:
        try:
            port_to_use = int(env_port)
            logger.info(f"Usando porta da vari√°vel de ambiente GRADIO_SERVER_PORT: {port_to_use}")
        except ValueError:
            logger.error(f"Valor inv√°lido para GRADIO_SERVER_PORT: '{env_port}'. Usando portas padr√£o.")
            port_to_use = 0 # Reseta para que tente as portas padr√£o
    
    if port_to_use > 0:
        try:
            demo.launch(share=False, server_name="0.0.0.0", server_port=port_to_use, show_error=True, debug=True, prevent_thread_lock=True, **auth_params)
            return # Sucesso
        except Exception as e_launch:
            logger.error(f"Erro ao usar porta {port_to_use} da vari√°vel de ambiente: {e_launch}", exc_info=True)
            raise # Relan√ßa se a porta especificada falhar
            
    ports_to_try = [8521, 8522, 8523, 7860, 7861] # Lista de portas para DEVELOP (8520 reservada para produ√ß√£o)
    logger.info(f"Tentando portas para o servidor Gradio: {ports_to_try}")
    for port_val in ports_to_try:
        try:
            logger.info(f"Tentando iniciar Gradio na porta {port_val}...")
            demo.launch(share=False, server_name="0.0.0.0", server_port=port_val, show_error=True, debug=True, prevent_thread_lock=True, **auth_params)
            break 
        except OSError as e_os:
            if "address already in use" in str(e_os).lower() or "cannot assign requested address" in str(e_os).lower():
                logger.warning(f"Porta {port_val} j√° est√° em uso ou endere√ßo n√£o pode ser atribu√≠do.")
            else:
                logger.error(f"OSError ao tentar iniciar Gradio na porta {port_val}: {e_os}", exc_info=True)
            if port_val == ports_to_try[-1]:
                logger.critical(f"Falha ao encontrar porta dispon√≠vel para Gradio.")
                raise
            logger.info("Tentando pr√≥xima porta...")
        except Exception as e_other_launch:
            logger.critical(f"Erro inesperado ao tentar iniciar Gradio na porta {port_val}: {e_other_launch}", exc_info=True)
            if port_val == ports_to_try[-1]: raise
            logger.info("Tentando pr√≥xima porta...")

async def main():
    logger.info("Iniciando RAG Chat Client...")
    print("Verificando conex√£o com servidores MCP...")
    all_servers_ok = True
    for name, config in MCP_SERVERS.items():
        print(f"Verificando {name}...")
        available, details = await check_server_availability(name, config["url"])
        if available:
            print(f"‚úì {name} OK. Ferramentas: {', '.join(details) if isinstance(details, list) else details}")
        else:
            print(f"‚úó {name} FALHA: {details}")
            all_servers_ok = False
    if not all_servers_ok:
        logger.warning("Um ou mais servidores MCP n√£o est√£o dispon√≠veis.")
    print("Configurando e iniciando interface Gradio...")
    setup_and_launch_gradio()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Aplica√ß√£o encerrada.")
    except Exception as e_fatal:
        logger.critical(f"Erro fatal: {e_fatal}", exc_info=True)